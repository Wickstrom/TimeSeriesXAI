{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NotebookECG200Experiment.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_LK4V466HkGA"},"source":["This notebook contains code for replicating experiments from the paper \"Uncertainty-Aware Deep Ensembles for Reliable and Explainable Predictions of Clinical Time Series\" on ECG200 dataset. See [this webiste](http://www.timeseriesclassification.com/description.php?Dataset=ECG200) for more info on downloading the data. Remember to enable the use of GPU to speed up computations.\n","\n","The following cell loads the packages used in this eexperiment, and mounts the drive. You need to change the \"path\" variable to the directory where the ecg data is stored in .arff format."]},{"cell_type":"code","metadata":{"id":"RbF3y1VGPRbM","cellView":"both","executionInfo":{"status":"ok","timestamp":1601556652447,"user_tz":-120,"elapsed":593,"user":{"displayName":"Kristoffer Wickstr√∏m","photoUrl":"","userId":"04547063320981780474"}},"outputId":"adfa9f8b-82e6-4f0e-ae33-16bed1d3cebe","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title Load packages and data\n","import os\n","import sys\n","import time\n","import copy\n","import random\n","import torch as th\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import torch.nn as nn\n","import matplotlib.cm as cm\n","import torch.nn.init as init\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","from itertools import product\n","from matplotlib.colors import Normalize\n","from sklearn.decomposition import PCA\n","from IPython.display import clear_output\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, StandardScaler\n","\n","drive.mount('/content/drive')\n","\n","path = 'INSERT YOUR PATH HERE'\n","\n","def to_np(x):\n","    return x.cpu().detach().numpy()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H603Xr5YLyXN"},"source":["The following cell defines a function that loads data in the .arff format. The function is based on the same function from the sktime libray found at [this website](https://github.com/alan-turing-institute/sktime)."]},{"cell_type":"code","metadata":{"id":"de5yyXcMLn-9","cellView":"form"},"source":["#@title Function for loading data in arff format. Code adopted from sktime library\n","\n","\n","def load_from_arff_to_dataframe(full_file_path_and_name, has_class_labels=True, return_separate_X_and_y=True, replace_missing_vals_with='NaN'):\n","\n","    instance_list = []\n","    class_val_list = []\n","\n","    data_started = False\n","    is_multi_variate = False\n","    is_first_case = True\n","\n","    with open(full_file_path_and_name, 'r') as f:\n","        for line in f:\n","\n","            if line.strip():\n","                if is_multi_variate is False and \"@attribute\" in line.lower() and \"relational\" in line.lower():\n","                    is_multi_variate = True\n","\n","                if \"@data\" in line.lower():\n","                    data_started = True\n","                    continue\n","\n","                # if the 'data tag has been found, the header information has been cleared and now data can be loaded\n","                if data_started:\n","                    line = line.replace(\"?\", replace_missing_vals_with)\n","\n","                    if is_multi_variate:\n","                        if has_class_labels:\n","                            line, class_val = line.split(\"',\")\n","                            class_val_list.append(class_val.strip())\n","                        dimensions = line.split(\"\\\\n\")\n","                        dimensions[0] = dimensions[0].replace(\"'\", \"\")\n","\n","                        if is_first_case:\n","                            for d in range(len(dimensions)):\n","                                instance_list.append([])\n","                            is_first_case = False\n","\n","                        for dim in range(len(dimensions)):\n","                            instance_list[dim].append(pd.Series([float(i) for i in dimensions[dim].split(\",\")]))\n","\n","                    else:\n","                        if is_first_case:\n","                            instance_list.append([])\n","                            is_first_case = False\n","\n","                        line_parts = line.split(\",\")\n","                        if has_class_labels:\n","                            instance_list[0].append(pd.Series([float(i) for i in line_parts[:len(line_parts)-1]]))\n","                            class_val_list.append(line_parts[-1].strip())\n","                        else:\n","                            instance_list[0].append(pd.Series([float(i) for i in line_parts[:len(line_parts)]]))\n","\n","    x_data = pd.DataFrame(dtype=np.float32)\n","    for dim in range(len(instance_list)):\n","        x_data['dim_' + str(dim)] = instance_list[dim]\n","\n","    if has_class_labels:\n","        if return_separate_X_and_y:\n","            return x_data, np.asarray(class_val_list)\n","        else:\n","            x_data['class_vals'] = pd.Series(class_val_list)\n","\n","    return x_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YISKnnm9m3Qq","cellView":"form"},"source":["#@title Dataset loader and create dataset\n","def load_dataset(split, path, return_X_y=True):\n","\n","    if split in [\"TRAIN\", \"TEST\"]:\n","        fname = 'ECG200_' + split + '.arff'\n","        abspath = os.path.join(path, fname)\n","        X, y = load_from_arff_to_dataframe(abspath)\n","\n","    y = LabelBinarizer().fit_transform(y)\n","    if y.shape[1] == 1: y = np.hstack((y, 1 - y))\n","\n","    X = pd.DataFrame(X).to_numpy()\n","    y = pd.DataFrame(y).to_numpy()\n","\n","    X = np.array(np.ndarray.tolist(X), dtype=np.float32)\n","    y = np.array(np.ndarray.tolist(y), dtype=np.int32)\n","\n","    where_are_NaNs = np.isnan(X)\n","    X[where_are_NaNs] = 0\n","\n","    scaler = StandardScaler()\n","\n","    for i in range(X.shape[1]):\n","        X[:, i, :] = scaler.fit_transform(X[:, i, :])\n","\n","    # Return appropriately\n","    if return_X_y:\n","        return X, y\n","    else:\n","        X['class_val'] = pd.Series(y)\n","        return X\n","\n","class TSDatasetSupervised(Dataset):\n","    def __init__(self, split, device='cuda'):\n","\n","        self.device = device\n","\n","        x, y = load_dataset(split=split, path=path, return_X_y=True)\n","\n","        self.x = th.tensor(x, dtype=th.float, device=device)\n","        self.y = th.tensor(y, dtype=th.long, device=device).argmax(1)\n","        self.n_c = 2\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","    def __getitem__(self, idx):      \n","        return self.x[idx], self.y[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7I56oITBNvQF"},"source":["The following cell creates the class for the convolutional neural network used in this experiment. The model is based on the fully convolutional network of Want et.al.: [Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline](https://arxiv.org/abs/1611.06455).\n","\n"]},{"cell_type":"code","metadata":{"id":"ru1_clhc_2bp","cellView":"form"},"source":["#@title Define CNN\n","\n","class CNN(nn.Module):\n","    def __init__(self, n_in, n_c):\n","        super(CNN, self).__init__()\n","\n","        n_hid = 128\n","        act = nn.ReLU()\n","\n","        self.l1 = nn.Sequential(\n","            nn.Conv1d(n_in, n_hid, kernel_size=7, padding=3),\n","            nn.BatchNorm1d(n_hid),\n","            act\n","        )\n","\n","        self.l2 = nn.Sequential(\n","            nn.Conv1d(n_hid, 2*n_hid, kernel_size=5, padding=2),\n","            nn.BatchNorm1d(2*n_hid),\n","            act\n","        )\n","\n","        self.l3 = nn.Sequential(\n","            nn.Conv1d(2*n_hid, n_hid, kernel_size=3, padding=1),\n","            nn.BatchNorm1d(n_hid),\n","            act\n","        )\n","\n","        self.gap = nn.Sequential(\n","            nn.AdaptiveAvgPool1d(1),\n","            nn.Flatten()\n","        )\n","\n","        self.out = nn.Linear(n_hid, n_c)\n","\n","    def forward(self, x):\n","\n","        l1 = self.l1(x)\n","        l2 = self.l2(l1)\n","        l3 = self.l3(l2)\n","\n","        gap = self.gap(l3)\n","        out = self.out(gap)\n","\n","        return out\n","\n","    def CAM(self, x):\n"," \n","        l1 = self.l1(x)\n","        l2 = self.l2(l1)\n","        l3 = self.l3(l2)\n","\n","        out = th.matmul(l3.transpose(2, 1), self.out.weight.T)\n","        out = nn.functional.relu(out)\n","        out = out.cpu().detach().numpy()\n","\n","        return out\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"joTJIJygOXD0"},"source":["The following cell creates function that are used throughout the rest of the code."]},{"cell_type":"code","metadata":{"id":"4BZz1JiQAWyU","cellView":"form"},"source":["#@title Define some useful functions\n","\n","def calc_metrics(y, y_hat):\n","\n","    TN, FP, FN, TP = confusion_matrix(y, y_hat).ravel()\n","\n","    precision = TP / (TP+FP)\n","    FallOut = FP / (FP+TN)\n","    recall = TP / (TP+FN)\n","    NPV = TN / (TN+FN)\n","\n","    return [precision, recall, NPV, FallOut]\n","\n","\n","def get_ensemble_pred(x, model_list):\n","\n","    for model_idx, model in enumerate(model_list):\n","        out = model(xte)\n","        if model_idx == 0: y_hat = to_np(th.nn.functional.softmax(out, dim=1))\n","        else: y_hat += to_np(th.nn.functional.softmax(out, dim=1))\n","\n","    y_hat = y_hat / len(model_list)\n","\n","    return y_hat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E_MvoVLbOSt4"},"source":["The following cell traings the single model and the ensemble model. In this code, \"M\" denotes the number of models in the ensemble, and \"num_repeats\" denotes the number of times the experiments will be repeated. This is set to 1 here, but in the paper we do 10 independet training runs.\n","\n"]},{"cell_type":"code","metadata":{"id":"T0judMNIsydP","cellView":"form"},"source":["#@title Experiment\n","\n","metrics_list = []\n","ensemble_metrics_list = []\n","\n","M, num_repeats = 10, 1\n","model_list = []\n","\n","\n","training_set = TSDatasetSupervised('TRAIN')\n","test_set = TSDatasetSupervised('TEST')\n","\n","batch_size_tr = len(training_set.x)\n","batch_size_te = len(test_set.x)\n","\n","training_generator = DataLoader(training_set, batch_size=batch_size_tr,\n","                                shuffle=True, drop_last=False)\n","test_generator = DataLoader(test_set, batch_size= batch_size_te,\n","                                shuffle=True, drop_last=False)\n","\n","for run in range(num_repeats):\n","    for m in range(M):\n","\n","        print(f\"Model number {m}\")\n","\n","        model = CNN(training_set.x.shape[1], training_set.n_c).to('cuda')\n","        criterion = th.nn.CrossEntropyLoss()\n","        optimizer = th.optim.Adam(model.parameters())\n","\n","        LossList, AccList, epochs = [], [], 150\n","\n","        for i in range(epochs):\n","            for xtr, ytr in training_generator:\n","\n","                model.train()\n","                optimizer.zero_grad()\n","\n","                xtr = xtr.to('cuda')\n","                out = model(xtr)\n","\n","                loss = criterion(out, ytr)\n","                loss.backward()\n","                LossList.append(loss.item())\n","                optimizer.step()\n","\n","            with th.no_grad():\n","                model.eval()\n","                for xte, yte in test_generator:\n","\n","                    out = model(xte)\n","                    y_hat = th.argmax(out, 1)\n","                    accuracy = th.eq(yte, y_hat).sum().float() / len(xte)\n","                    AccList.append(accuracy.item())\n","\n","        model_list.append(copy.deepcopy(model))\n","        metrics_list.append(calc_metrics(to_np(yte), to_np(y_hat)))\n","        print(metrics_list[-1])\n","\n","    y_hat_ensemble = get_ensemble_pred(xte, model_list)\n","    ensemble_metrics_list.append(calc_metrics(to_np(yte), y_hat_ensemble.argmax(1)))\n","    print(ensemble_metrics_list[-1])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OtVHd9VuOjig"},"source":["The following cell makes a prediction for a random sample in the test data and plots the time steps that are most relevant for the prediction, and the uncertainty of the relevance scores.\n","\n"]},{"cell_type":"code","metadata":{"id":"RJSc_tvZsy5h","cellView":"form"},"source":["#@title CAM illustration\n","\n","model.eval()\n","N, T = test_set.x.shape[0], test_set.x.shape[-1]\n","sample = np.random.choice(list(np.arange(0, N, 1)), 1)[0]\n","\n","my_cmap_mean = cm.get_cmap('Greens')\n","my_cmap_std = cm.get_cmap('Reds')\n","sm_mean = plt.cm.ScalarMappable(cmap=my_cmap_mean, norm=Normalize(vmin=0, vmax=1))\n","sm_std = plt.cm.ScalarMappable(cmap=my_cmap_std, norm=Normalize(vmin=0, vmax=1))\n","\n","x_plot = np.arange(0, T, 1)\n","\n","interpret_array =  np.array([model.CAM(test_set.x) for model in model_list])\n","interpret_array = interpret_array / (interpret_array.max(1, keepdims=True)+1.e-8)\n","\n","interpret_mean = interpret_array.mean(0)\n","interpret_std = interpret_array.std(0)\n","\n","pred = get_ensemble_pred(test_set.x, model_list).argmax(1)\n","y_samp = to_np(test_set.y[sample])\n","pred_samp = pred[sample]\n","\n","scale_col_mean = np.abs(interpret_mean[sample, :, pred_samp] / (np.abs(interpret_mean[sample, :, pred_samp]).max())).reshape(T, 1)\n","color_weight_mean = np.ones((T, 3))\n","color_weight_mean = np.concatenate((color_weight_mean, scale_col_mean), 1)\n","plot_col_mean = my_cmap_mean(interpret_mean[sample, :, pred_samp])*color_weight_mean\n","\n","plt.figure(1, figsize=(15, 5))\n","plt.plot(x_plot, to_np(test_set.x[sample])[0], color='black')\n","plt.bar(x_plot, 200*np.ones_like(x_plot), bottom=-100, width=1.0,\n","        color=plot_col_mean)\n","plt.xlim(0, T)\n","plt.ylim(to_np(test_set.x[sample])[0].min(), to_np(test_set.x[sample])[0].max())\n","plt.title(f\"Mean relevance score. True: {y_samp}. Predicted: {pred_samp}\")\n","plt.ylabel('mV')\n","plt.xlabel('ms')\n","plt.show()\n","\n","scale_col_std = np.abs(interpret_std[sample, :, pred_samp] / (np.abs(interpret_std[sample, :, pred_samp]).max())).reshape(T, 1)\n","color_weight_std = np.ones((T, 3))\n","color_weight_std = np.concatenate((color_weight_std, scale_col_std), 1)\n","plot_col_std = my_cmap_std(interpret_std[sample, :, pred_samp])*color_weight_std\n","\n","plt.figure(2, figsize=(15, 5))\n","plt.plot(x_plot, to_np(test_set.x[sample])[0], color='black')\n","plt.bar(x_plot, 200*np.ones_like(x_plot), bottom=-100, width=1.0,\n","        color=plot_col_std)\n","plt.xlim(0, T)\n","plt.ylim(to_np(test_set.x[sample])[0].min(), to_np(test_set.x[sample])[0].max())\n","plt.title(f\"Uncertainty of relevance score. True: {y_samp}. Predicted: {pred_samp}\")\n","plt.ylabel('mV')\n","plt.xlabel('ms')\n","plt.show()\n","\n","filtered_time_steps = (scale_col_std.squeeze() < scale_col_std.squeeze().mean())*1.0\n","scale_col_filtered = np.reshape(scale_col_mean.squeeze() * filtered_time_steps, (T, 1))\n","color_weight_filtered = np.ones((T, 3))\n","color_weight_filtered = np.concatenate((color_weight_filtered, scale_col_filtered), 1)\n","plot_col_filtered = my_cmap_mean(interpret_mean[sample, :, pred_samp])*color_weight_filtered\n","\n","\n","plt.figure(3, figsize=(15, 5))\n","plt.plot(x_plot, to_np(test_set.x[sample])[0], color='black')\n","plt.bar(x_plot, 200*np.ones_like(x_plot), bottom=-100, width=1.0,\n","        color=plot_col_filtered)\n","plt.xlim(0, T)\n","plt.ylim(to_np(test_set.x[sample])[0].min(), to_np(test_set.x[sample])[0].max())\n","plt.ylabel('mV')\n","plt.xlabel('ms')\n","plt.title(f\"Uncertainty-filtered mean relevance score. True: {y_samp}. Predicted: {pred_samp}\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ca4PFyiPg_K"},"source":["The following cell computes the consistency for single and ensemble model. This cell takes some time to run."]},{"cell_type":"code","metadata":{"id":"RpVdC1AsiI2c","cellView":"form"},"source":["#@title Compute consistency over independent training runs\n","\n","\n","num_repeats, M = 10, 10\n","topk_list = [5, 7, 10, 15]\n","\n","training_set = TSDatasetSupervised('TRAIN')\n","test_set = TSDatasetSupervised('TEST')\n","\n","batch_size_tr = len(training_set.x)\n","batch_size_te = len(test_set.x)\n","\n","training_generator = DataLoader(training_set, batch_size=batch_size_tr,\n","                                shuffle=True, drop_last=False)\n","test_generator = DataLoader(test_set, batch_size= batch_size_te,\n","                                shuffle=True, drop_last=False)\n","\n","single_model_rel_scores = np.zeros((num_repeats, 100, test_set.x.shape[-1]))\n","ensemble_model_rel_scores = np.zeros((num_repeats, 100, test_set.x.shape[-1]))\n","\n","for repeat_n in range(num_repeats):\n","\n","    model_list = []\n","\n","    print(f\"Run number {repeat_n}\")\n","\n","    for m in range(M):\n","\n","        print(f\"Model number {m}\")\n","\n","        model = CNN(training_set.x.shape[1], training_set.n_c).to('cuda')\n","        criterion = th.nn.CrossEntropyLoss()\n","        optimizer = th.optim.Adam(model.parameters())\n","\n","        LossList, AccList, epochs = [], [], 150\n","\n","        for i in range(epochs):\n","            for xtr, ytr in training_generator:\n","\n","                model.train()\n","                optimizer.zero_grad()\n","\n","                xtr = xtr.to('cuda')\n","                out = model(xtr)\n","\n","                loss = criterion(out, ytr)\n","                loss.backward()\n","                LossList.append(loss.item())\n","                optimizer.step()\n","\n","            with th.no_grad():\n","                model.eval()\n","                for xte, yte in test_generator:\n","\n","                    out = model(xte)\n","                    y_hat = th.argmax(out, 1)\n","                    accuracy = th.eq(yte, y_hat).sum().float() / len(xte)\n","                    AccList.append(accuracy.item())\n","\n","        model_list.append(copy.deepcopy(model))\n","\n","    interpret_array = model_list[np.random.randint(0, len(model_list), 1)[0]].CAM(test_set.x)\n","    single_model_rel_scores[repeat_n] = np.take_along_axis(interpret_array,\n","                                                           to_np(test_set.y[:, None, None]), axis=2).squeeze()\n","\n","\n","    interpret_array_ensemble = np.array([model.CAM(test_set.x) for model in model_list])\n","    interpret_array_ensemble = interpret_array_ensemble.mean(0)\n","\n","    ensemble_model_rel_scores[repeat_n] = np.take_along_axis(interpret_array_ensemble,\n","                                                             to_np(test_set.y[:, None, None]), axis=2).squeeze()\n","\n","    clear_output()\n","\n","for topk_idx, topk in enumerate(topk_list):\n","\n","    interpret_idx = single_model_rel_scores.argsort(2)[:, :, -topk:]\n","    sim_array = np.zeros((num_repeats, num_repeats))\n","\n","    for i in range(num_repeats):\n","        for j in range(num_repeats):\n","            sim_array[i, j] = sum([np.isin(interpret_idx[i, n], interpret_idx[j, n]).sum() / topk for n in range(len(test_set.x))]) / len(test_set.x)\n","\n","    print(f\"Consistency for single model: {sim_array.mean()}\")\n","\n","for topk_idx, topk in enumerate(topk_list):\n","\n","    interpret_idx = ensemble_model_rel_scores.argsort(2)[:, :, -topk:]\n","    sim_array = np.zeros((num_repeats, num_repeats))\n","\n","    for i in range(num_repeats):\n","        for j in range(num_repeats):\n","            sim_array[i, j] = sum([np.isin(interpret_idx[i, n], interpret_idx[j, n]).sum() / topk for n in range(len(test_set.x))]) / len(test_set.x)\n","\n","    print(f\"Consistency for ensemble model: {sim_array.mean()}\")"],"execution_count":null,"outputs":[]}]}